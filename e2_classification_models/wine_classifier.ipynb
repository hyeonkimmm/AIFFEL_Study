{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Classifer\n",
    "---\n",
    "#### 주요 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine # load wine data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix # Use confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier # Use DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Use RandomForestClassifier\n",
    "from sklearn import svm # Use Support Vector Machine(SVM)\n",
    "from sklearn.linear_model import SGDClassifier # Use Stochastic Gradient Descent Classifier (SGDClassifier)\n",
    "from sklearn.linear_model import LogisticRegression # Use LogisticRegression\n",
    "from sklearn.metrics import accuracy_score # 정확도 확인\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 이해하기\n",
    "---\n",
    "- Feature Data 지정하기\n",
    "- Label Data 지정하기\n",
    "- Target Names 출력해 보기\n",
    "- 데이터 Describe 해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n",
      "wine_data.shape: (178, 13)\n",
      "wine_label.shape: (178,)\n",
      "wine.target_names: ['class_0' 'class_1' 'class_2']\n",
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "..       ...         ...   ...                ...        ...            ...   \n",
      "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
      "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
      "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
      "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
      "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
      "\n",
      "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0          3.06                  0.28             2.29             5.64  1.04   \n",
      "1          2.76                  0.26             1.28             4.38  1.05   \n",
      "2          3.24                  0.30             2.81             5.68  1.03   \n",
      "3          3.49                  0.24             2.18             7.80  0.86   \n",
      "4          2.69                  0.39             1.82             4.32  1.04   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "173        0.61                  0.52             1.06             7.70  0.64   \n",
      "174        0.75                  0.43             1.41             7.30  0.70   \n",
      "175        0.69                  0.43             1.35            10.20  0.59   \n",
      "176        0.68                  0.53             1.46             9.30  0.60   \n",
      "177        0.76                  0.56             1.35             9.20  0.61   \n",
      "\n",
      "     od280/od315_of_diluted_wines  proline  label  \n",
      "0                            3.92   1065.0      0  \n",
      "1                            3.40   1050.0      0  \n",
      "2                            3.17   1185.0      0  \n",
      "3                            3.45   1480.0      0  \n",
      "4                            2.93    735.0      0  \n",
      "..                            ...      ...    ...  \n",
      "173                          1.74    740.0      2  \n",
      "174                          1.56    750.0      2  \n",
      "175                          1.56    835.0      2  \n",
      "176                          1.62    840.0      2  \n",
      "177                          1.60    560.0      2  \n",
      "\n",
      "[178 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "wine = load_wine() #Load Data\n",
    "print(wine.keys()) #wine Data\n",
    "wine_data = wine.data\n",
    "print(\"wine_data.shape: {}\".format(wine_data.shape)) \n",
    "wine_label = wine.target\n",
    "print(\"wine_label.shape: {}\".format(wine_label.shape))\n",
    "print(\"wine.target_names: {}\".format(wine.target_names))\n",
    "#print(wine.DESCR) #정보 출력\n",
    "print(wine.feature_names)\n",
    "wine_df = pd.DataFrame(data=wine_data, columns=wine.feature_names)\n",
    "wine_df[\"label\"] = wine_label\n",
    "print(wine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train,test 데이터 분리\n",
    "---\n",
    "데이터 분리를 위해 sklearn.model_selection 사용, 추가로 shuffle 속성을 True로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine_data, \n",
    "                                                    wine_label, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=5,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다양한 모델로 학습시켜보기\n",
    "---\n",
    "- Decision Tree 사용해 보기\n",
    "- Random Forest 사용해 보기\n",
    "- SVM 사용해 보기\n",
    "- SGD Classifier 사용해 보기\n",
    "- Logistic Regression 사용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree accuarcy: 0.8611111111111112\n",
      "RandomForestClassifier accuarcy: 1.0\n",
      "Support Vector Machine accuarcy: 0.6944444444444444\n",
      "SGDClassifier accuarcy: 0.5833333333333334\n",
      "LogisticRegression accuarcy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Use DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=32)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_DC = decision_tree.predict(X_test)\n",
    "DC_accuary = accuracy_score(y_test,y_pred_DC)\n",
    "print(\"DecisionTree accuarcy: {}\".format(DC_accuary))\n",
    "\n",
    "# Use RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(random_state=32)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred_RF = random_forest.predict(X_test)\n",
    "RF_accuary = accuracy_score(y_test,y_pred_RF)\n",
    "print(\"RandomForestClassifier accuarcy: {}\".format(RF_accuary))\n",
    "\n",
    "# Use Support Vector Machine(SVM)\n",
    "clf = svm.SVC(random_state=32)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_SVM = clf.predict(X_test)\n",
    "SVM_accuary = accuracy_score(y_test,y_pred_SVM)\n",
    "print(\"Support Vector Machine accuarcy: {}\".format(SVM_accuary))\n",
    "\n",
    "# Use Stochastic Gradient Descent Classifier (SGDClassifier)\n",
    "clf = SGDClassifier(loss=\"perceptron\").fit(X_train, y_train)\n",
    "y_pred_SGDC = clf.predict(X_test)\n",
    "SGDC_accuary = accuracy_score(y_test,y_pred_SGDC)\n",
    "print(\"SGDClassifier accuarcy: {}\".format(SGDC_accuary))\n",
    "\n",
    "# Use LogisticRegression\n",
    "logistic_model = LogisticRegression(max_iter=3000)\n",
    "clf = logistic_model.fit(X_train, y_train)\n",
    "y_pred_LOGR = clf.predict(X_test)\n",
    "LOGR_accuary = accuracy_score(y_test,y_pred_LOGR)\n",
    "print(\"LogisticRegression accuarcy: {}\".format(LOGR_accuary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.87        17\n",
      "           1       0.73      1.00      0.85        11\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.87      0.88      0.86        36\n",
      "weighted avg       0.89      0.86      0.86        36\n",
      "\n",
      "Use RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "Use Support Vector Machine(SVM)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.71      0.77        17\n",
      "           1       0.69      0.82      0.75        11\n",
      "           2       0.44      0.50      0.47         8\n",
      "\n",
      "    accuracy                           0.69        36\n",
      "   macro avg       0.66      0.67      0.66        36\n",
      "weighted avg       0.72      0.69      0.70        36\n",
      "\n",
      "Use Stochastic Gradient Descent Classifier (SGDClassifier)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79        17\n",
      "           1       0.67      0.36      0.47        11\n",
      "           2       0.29      0.50      0.36         8\n",
      "\n",
      "    accuracy                           0.58        36\n",
      "   macro avg       0.59      0.54      0.54        36\n",
      "weighted avg       0.65      0.58      0.60        36\n",
      "\n",
      "Use LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        17\n",
      "           1       0.85      1.00      0.92        11\n",
      "           2       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.95      0.94      0.94        36\n",
      "weighted avg       0.95      0.94      0.95        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Use DecisionTreeClassifier\")\n",
    "print(classification_report(y_test, y_pred_DC))\n",
    "print(\"Use RandomForestClassifier\")\n",
    "print(classification_report(y_test, y_pred_RF))\n",
    "print(\"Use Support Vector Machine(SVM)\")\n",
    "print(classification_report(y_test, y_pred_SVM))\n",
    "print(\"Use Stochastic Gradient Descent Classifier (SGDClassifier)\")\n",
    "print(classification_report(y_test, y_pred_SGDC))\n",
    "print(\"Use LogisticRegression\")\n",
    "print(classification_report(y_test, y_pred_LOGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "---\n",
    "Wine 분류기의 경우 precsion , recall 의 중요성이 명확하게 드러나지 않는다고 생각합니다\n",
    "\n",
    "Wine 분류기에선 각 모델마다 편차가 꽤 심한 편인데 그중에 RandomForeset와 LogisticRegression의 Accuracy가 \n",
    "\n",
    "높게 나왔음을 알 수 있습니다. 특히 RandomForeset는 100%의 놀라운 Accuracy를 보이고 있는데,\n",
    "\n",
    "이는 RandomForest 특성상 각각의 특성들을 Random하게 모두 비교하기 때문에 13가지의 특성을 가지고 있는 wine\n",
    "\n",
    "data를 분류하는데에 적합했다고 생각합니다.\n",
    "\n",
    "cf.각각의 모델의 원리를 아직은 정확하게 알지 못하지만 이해하는데로 평가Comment를 추가로 업로드 하겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
